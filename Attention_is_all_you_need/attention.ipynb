{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi_6SKl80ifS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2  # 문장2개\n",
        "str_len =4  #문장 길이 4\n",
        "vocab_size = 100 # 전체 단어 개수\n",
        "d_model = 512 # 임베딩 벡터 차원"
      ],
      "metadata": {
        "id": "6gVEV7EX1FGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layer의 가장 큰 특징은 바로 문장의 길이를 가변적으로 가져갈 수 있다는 것인데, 왜 str_len을 고정으로 두는 걸까?\n",
        "-> GPU 내에서 배치 단위로 묶어서 병렬 연산을 하고, 추후 위치 인코딩을 위해서 우선은 문장의 길이를 가변적으로 가져간다!"
      ],
      "metadata": {
        "id": "SZirZ5114yUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"I love AI <pad>\" -> [1,2,3,0]\n",
        "# \"Hello world <pad> <pad>\" -> [4.5.0.0]"
      ],
      "metadata": {
        "id": "cs_qmFLg1MSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.LongTensor([\n",
        "    [1,2,3,0],\n",
        "    [4,5,0,0]\n",
        "])"
      ],
      "metadata": {
        "id": "mE7AyxHc12vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input Shape: {input_data.shape}\") #(Batch_Size, Seq_Len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0NqwUju2DIc",
        "outputId": "2894f6bc-4ab0-4782-f6ed-5bac4065e42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = nn.Embedding(\n",
        "    num_embeddings = vocab_size,\n",
        "    embedding_dim = d_model\n",
        "    )"
      ],
      "metadata": {
        "id": "IAE0WZs12jBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_data = embedding_layer(input_data)"
      ],
      "metadata": {
        "id": "fJjfUbwN260r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embedded Shape: {embedded_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SWvbbTM3DzG",
        "outputId": "6f9cdcde-d354-4fc1-891d-1eed452ba28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Shape: torch.Size([2, 4, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[문장 개수,문장 길이]\n",
        "->[문장 개수, 문장 길이, 512]"
      ],
      "metadata": {
        "id": "Wd9_46g45UFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Position Encoding"
      ],
      "metadata": {
        "id": "51PAIJDe6_Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model = 512, max_len = str_len, dropout = 0.1):\n",
        "    '''\n",
        "    d_model: 임베딩 차원(기본값 512),\n",
        "    max_len: 모델이 받아들일 수 있는 최대 문장 길이\n",
        "    dropout: 과적합 방지\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, d_model) #[str_len, 512]\n",
        "\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #위치 인덱스 생성\n",
        "\n",
        "    # 주기를 결정하는 분모 계산(10000^(2i/d_model))\n",
        "    div_term = torch.exp(0,d_model,2).float() * (-math.log(10000.0)/d_model)\n",
        "\n",
        "    #짝수 인덱스는 Sin\n",
        "    pe[:,0::2] = torch.sin(position * div_term)\n",
        "\n",
        "    #홀수 인덱스는 Cos\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    #차원 맞추기\n",
        "    '''\n",
        "    (batch_size,str_len,d_model)과 더하기 위해 pe의 차원을\n",
        "    (1.str_len,d_model)로 바꾼다.\n",
        "    '''\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x + self.pe[:,:x.size(1)]\n",
        "\n",
        "      return self.dropout(x)"
      ],
      "metadata": {
        "id": "m8WkuWwn5Z_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerInput(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, max_len, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model) #보정\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZFuhjafRAm_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 아무튼 지금까지의 차원은 [문장 개수, 문장 길이, 512]"
      ],
      "metadata": {
        "id": "SCAdqRFSCmQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_head):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model #512\n",
        "    self.n_head = n_head #논문은 일단 8개.\n",
        "    self.d_k = d_model // n_head #각 헤드의 차원 (정수 나누기로 변경)\n",
        "\n",
        "    assert d_model % n_head == 0\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    batch_size = q.size(0)\n",
        "\n",
        "    q = self.w_q(q)\n",
        "    k = self.w_k(k)\n",
        "    v = self.w_v(v)\n",
        "\n",
        "    # [batch, seq_len , d_model] -> [batch, seq_len, n_head, d_k] -> [batch, n_head, seq_len, d_k]\n",
        "    q = q.view(batch_size, -1, self.n_head, self.d_k).transpose(1,2)\n",
        "    k = k.view(batch_size, -1, self.n_head, self.d_k).transpose(1,2)\n",
        "    v = v.view(batch_size, -1, self.n_head, self.d_k).transpose(1,2)\n",
        "\n",
        "    scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask==0.-1e9)\n",
        "\n",
        "    attn_weights = torch.softmax(scores,dim=-1)\n",
        "\n",
        "    output = torch.matmul(attn_weights, v) #[batch, n_head, seq_len, d_k]\n",
        "\n",
        "    output = output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
        "    #[batch, seq_len, n_head, d_k] -> [batch,seq_len, d_model]\n",
        "\n",
        "    output = self.w_o(output)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "mTs_aLavCtFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646023a1"
      },
      "source": [
        "### Self-Attention에서의 Q, K, V 생성\n",
        "\n",
        "`MultiHeadAttention` 클래스에서 `forward` 메서드의 `q`, `k`, `v` 인자는 Self-Attention의 경우 모두 동일한 입력(예: 인코더의 경우 입력 임베딩)을 받게 됩니다. 각 `w_q`, `w_k`, `w_v` 선형 레이어가 이 동일한 입력을 각각 Q, K, V로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9600d27a",
        "outputId": "76d386d9-e54c-4991-d79e-cb42ca9e4af5"
      },
      "source": [
        "n_head = 8 # 예시로 헤드 개수를 8개로 설정\n",
        "multi_head_attn = MultiHeadAttention(d_model, n_head)\n",
        "\n",
        "# Self-Attention에서는 Q, K, V가 모두 동일한 embedded_data로부터 파생됩니다.\n",
        "# 즉, embedded_data를 각각 Q, K, V의 입력으로 사용합니다.\n",
        "output_attn, attn_weights = multi_head_attn(embedded_data, embedded_data, embedded_data)\n",
        "\n",
        "print(f\"Output of MultiHeadAttention (Self-Attention): {output_attn.shape}\")\n",
        "print(f\"Attention Weights Shape: {attn_weights.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of MultiHeadAttention (Self-Attention): torch.Size([2, 4, 512])\n",
            "Attention Weights Shape: torch.Size([2, 8, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#멀티 헤드 어텐션\n",
        "[문장 개수, 문장 길이, d_model]\n",
        "-> [문장 개수, 문장 길이, 헤드 개수, 헤드 별 차원]\n",
        "#헤드 별 차원: d_model / 헤드 개수\n",
        "-> [문장 개수, 헤드 개수, 문장 길이, 헤드 별 차원]\n",
        "멀티 헤드 어텐션 연산 이후\n",
        "\n",
        "[문장 개수, 문장 길이, d_model]"
      ],
      "metadata": {
        "id": "1ioVsdPR0TEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sub_layer_output):\n",
        "\n",
        "    #sub_layer_output: 어텐션이나 FFN을 통과하고 나온 결과값.\n",
        "    sub_layer_output = self.dropout(sub_layer_output)\n",
        "\n",
        "    output = x + sub_layer_output\n",
        "\n",
        "    #정규화\n",
        "    output = self.layer_norm(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "BsEDtVK95d0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Nom vs Layer Nom:\n",
        "Batch Nom -> 모든 문장의 첫 번째 단어끼리 평균\n",
        "Layer Nom -> 한 문장 혹은 벡터 안에서 평균\n",
        "\n",
        "#Post-Ln vs Pre-LN:\n",
        "Post LN: 높은 성능, 하지만 어려운 초기화\n",
        "Pre LN; 안정적인 성능"
      ],
      "metadata": {
        "id": "Y-po1Kr37pI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#왜? 512->2048로 늘려서 ReLU 및 Dropout을 하는가?\n",
        "- 영역을 넓혀서 복잡한 패턴 파악\n",
        "- 이후 ReLU(비선형 함수)로 표현력을 키운다"
      ],
      "metadata": {
        "id": "3IHIrUr1F7Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.w_1 = nn.Linear(d_model, d_ff) # 512 -> 2048\n",
        "    self.w_2 = nn.Linear(d_ff, d_model) # 2048 -> 512\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.w_1(x) #512 -> 2048\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.w_2(x) #2048 -> 512\n",
        "    return x"
      ],
      "metadata": {
        "id": "pbUKIBSw8KwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        " def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn =  MultiHeadAttention(d_model, n_head)\n",
        "\n",
        "    self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    self.sublayer1 = AddNorm(d_model, dropout) #어텐션 뒤에\n",
        "    self.sublayer2 = AddNorm(d_model, dropout) #FFN 뒤에\n",
        "\n",
        " def forward(self, x, mask):\n",
        "\n",
        "   attn_output, _ = self.self_attn(x,x,x,mask) #어텐션 수행, 인코더의 mask는 패딩용!\n",
        "   x = self.sublayer1(x, attn_output)# Reisudal 및 Normalization\n",
        "\n",
        "   ffn_output = self.feed_forward(x)\n",
        "   x = self.sublayer2(x, ffn_output)\n",
        "\n",
        "   return x"
      ],
      "metadata": {
        "id": "XWLArOamCpgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "Qz2KQ4MfE-Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_head, d_ff, max_len , n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_embedding = TransformerInput(vocab_size, d_model, max_len, dropout)\n",
        "\n",
        "    self.layers = get_clones(EncoderLayer(d_model, n_head, d_ff, dropout), n_layers)\n",
        "\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    x = self.input_embedding(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "zN6KJ2tTHbk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##디코더\n",
        "- 일단 임베딩에 쓰이는 Class를 재탕할 수 있다!\n",
        "\n",
        "- 디코더의 목표는 정답 생성이므로 한칸 밀린(Shifted Right) 문장이 들어가게 된다."
      ],
      "metadata": {
        "id": "QZSaQ2N3KhzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "인코더에 넣을 마스크를 생성할 함수.\n",
        "'''\n",
        "def generate_sqaure_subsequent_mask(sz):\n",
        "  '''\n",
        "  sz: 문장의 길이\n",
        "  '''\n",
        "  mask = torch.ones(sz,sz) #  sz * sz 크기의 모든 원소가 1인 행렬.\n",
        "\n",
        "  mask = torch.tril(mask)\n",
        "  #하삼각행렬 형태로 mask를 만듦.\n",
        "\n",
        "  return mask\n"
      ],
      "metadata": {
        "id": "48FO63gWKnpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_head, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    #마스크가 들어간 어텐션\n",
        "    self.self_attn = MultiHeadAttention(d_model, n_head)\n",
        "    #인코더의 정보를 보는 어텐션\n",
        "    self.enc_dec_attn = MultiHeadAttention(d_model, n_head)\n",
        "\n",
        "    self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    self.sublayer1 = AddNorm(d_model, dropout)\n",
        "    self.sublayer2 = AddNorm(d_model, dropout)\n",
        "    self.sublayer3 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask, memory_mask):\n",
        "      # tgt : 디코더 입력\n",
        "      # memory : 인코더 결과값(K,V가 enc_dec_attn에서 쓰임)\n",
        "      # tgt_mask : 마스크\n",
        "      # memory_mask : 인코더의 패딩용 마스크\n",
        "\n",
        "      #Masked Multi- Head Attn\n",
        "      tgt2, _ = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "      tgt = self.sublayer1(tgt, tgt2)\n",
        "\n",
        "      #Enc-Dec Attn\n",
        "      tgt2, _ = self.enc_dec_attn(tgt, memory, memory, memory_mask)\n",
        "      tgt = self.sublayer2(tgt, tgt2)\n",
        "\n",
        "      # FFN\n",
        "      tgt2 = self.feed_forward(tgt)\n",
        "      tgt = self.sublayer3(tgt, tgt2)\n",
        "\n",
        "      return tgt\n"
      ],
      "metadata": {
        "id": "jQtfjCLBMT9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#디코더의 인풋, 그리고 마스크.\n",
        "디코더는 다음 토큰을 예측하기 위해 실제로 '한 칸 밀린' 타겟 시퀀스를 입력으로 받습니다. 그리고 이 입력의 self-attention 과정에서는 '룩-어헤드 마스크'를 적용하여 현재 시점 이후의 토큰을 참조하지 못하게 함으로써, 진정한 의미의 순차적 예측을 가능하게 합니다. 두 가지 메커니즘이 합쳐져서 디코더가 문장을 생성하는 데 필요한 조건을 만족시킵니다.\n",
        "- Shifted Right:모델이 시퀀스의 각 위치에서 다음 토큰(단어)을 예측하도록 학습\n",
        "- Mask: 현재 예측하려는 위치의 토큰이 미래의 토큰을 보지 못하도록(미래 정보를 참조하지 못하도록) 강제합니다. 만약 현재 위치에서 미래의 토큰을 볼 수 있다면, 모델은 그저 미래 토큰을 복사하는 것과 같아져서 실제로 예측하는 능력을 학습할 수 없게 됩니다."
      ],
      "metadata": {
        "id": "xTrFG1lIRtok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_head, d_ff, max_len, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_embedding = TransformerInput(vocab_size, d_model, max_len, dropout)\n",
        "    self.layers = get_clones(DecoderLayer(d_model, n_head, d_ff, dropout), n_layers)\n",
        "\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,tgt,memory, tgt_mask, memory_mask):\n",
        "    x = self.input_embedding(tgt)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, tgt_mask, memory_mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "OwPstQlSSLQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_head, d_ff, src_max_len, tgt_max_len, n_layers, dropout, src_pad_idx, tgt_pad_idx):\n",
        "    super().__init__()\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    self.encoder = Encoder(src_vocab_size, d_model, n_head, d_ff, src_max_len, n_layers, dropout)\n",
        "    self.decoder = Decoder(tgt_vocab_size, d_model, n_head, d_ff, tgt_max_len, n_layers, dropout)\n",
        "\n",
        "    #출력 프로젝션: d_model -> vocab_size\n",
        "    self.projection = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    self._init_weights() #가중치 초기화: Xavier Initialization\n",
        "\n",
        "   def _init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "   def make_src_mask(self, src):\n",
        "    # src : [batch. src_len]\n",
        "    # 인코더에서 SelfAttention의 마스킹 부분을 땜빵하기 위해 쓰이는 패딩.\n",
        "    return (src != self.src_pad_idx).unsqueeze(1).usqueeze(2)\n",
        "    # [batch,1,1,src_len] 형태에서 브로드캐스팅.\n",
        "\n",
        "   def make_tgt_mask(self, tgt):\n",
        "    # tgt : [batch, tgt_len]\n",
        "\n",
        "    # 패딩 마스크(문장 끝부분 가리기) & [batch, 1, 1, tgt_len]\n",
        "    tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # 룩 - 어헤드 마스크 (미래 단어 가리기)\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_sub_mask = torch.tril(torch.ones(tgt_len, tgt_len, device = self.device)).bool()\n",
        "\n",
        "    # [batch, 1, tgt_len, tgt_len]\n",
        "    tgt_mask = tgt_pad_mask & tgt_sub_mask #패딩이 아니면서 & 현재보다 과거의 것만 True\n",
        "\n",
        "    return tgt_mask\n",
        "\n",
        "   def forward(self, src, tgt):\n",
        "\n",
        "    src_mask = self.mask_src_mask(src)\n",
        "    tgt_mask = self.mask_tgt_mask(tgt)\n",
        "\n",
        "    enc_output = self.encoder(src, src_mask)\n",
        "    dec_output = self.decoder(tgt,enc_output, tgt_mask, src_mask)\n",
        "\n",
        "    output = self.projection(dec_output) #[batch, sequence_length, d_model]  -> [batch,sequence_length, tgt_vocab_size]\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "lnQi4DGjT9yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 모델 | n_layers (쌓은 층수) | d_model (벡터 차원) | n_head (헤드 개수) | 파라미터 수 |\n",
        "|------|---------------------|--------------------|--------------------|------------|\n",
        "| Original Transformer | 6 | 512 | 8 | 65M |\n",
        "| BERT-Base | 12 | 768 | 12 | 110M |\n",
        "| BERT-Large | 24 | 1024 | 16 | 340M |"
      ],
      "metadata": {
        "id": "jAp4ZkLKC_4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT"
      ],
      "metadata": {
        "id": "3Yb7BzEs5G9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GELU랑 사전 정규화는 여건상 구현을 못햇음..."
      ],
      "metadata": {
        "id": "G2oYoSeUH1F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, max_len, dropout=0.1):\n",
        "      super().__init__()\n",
        "\n",
        "      #1. Token Embedding(기존과 동일)\n",
        "      self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "      #2. Position Embedding\n",
        "      self.position_embedding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "      #3. Segment Embedding\n",
        "      self.segment_embedding = nn.Embedding(2, d_model)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "   def forward(self, input_ids, segment_ids):\n",
        "     # input_ids : [batch, seq_len]\n",
        "     # segment_ids : [batch, seq_len]\n",
        "     x = self.token_embedding(input_ids) +  self.segment_embedding(segment_ids)\n",
        "\n",
        "     x = self.position_embedding(x)\n",
        "\n",
        "     return self.dropout(x)"
      ],
      "metadata": {
        "id": "SoCHtjxV5IZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model = 768, n_head = 12, d_ff = 3072,\n",
        "               max_len = 512, n_layers = 12, dropout = 0.1):\n",
        "\n",
        "    self.embeddings = BertEmbeddings(vocab_size, d_model, max_len, dropout)\n",
        "\n",
        "    self.encoder = Encoder(vocab_size, d_model, n_head, d_ff,\n",
        "                           max_len, n_layers, dropout)\n",
        "    #[CLS] 토큰 처리용\n",
        "    self.pooler_dense = nn.Linear(d_model, d_model)\n",
        "    self.pooler_activation = nn.Tanh()\n",
        "  def forward(self, input_ids, segment_ids, mask = None):\n",
        "    #임베딩\n",
        "    x = self.embeddings(input_ids, segment_ids)\n",
        "\n",
        "    #인코더 스택 12층\n",
        "    for layer in self.encoder.layers:\n",
        "      x = layer(x,mask)\n",
        "\n",
        "    #[CLS] 토큰 가공\n",
        "    cls_token = x[:,0]\n",
        "    pooled_output = self.pooler_activation(self.pooler_ense(cls_token))\n",
        "\n",
        "    return x, pooled_output"
      ],
      "metadata": {
        "id": "76dthlV5GsTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
